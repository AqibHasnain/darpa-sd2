{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np;\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math;\n",
    "import random;\n",
    "import matplotlib.pyplot as plt;\n",
    "import pickle\n",
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ydata shape:(480, 4110)\n",
      "Udata shape:(480, 2)\n"
     ]
    }
   ],
   "source": [
    "file_obj = open('NAND_Titration_IO_data_empty_landing_pads.pickle','rb')\n",
    "allVars = pickle.load(file_obj)\n",
    "Udata = allVars[0]  #Output Data \n",
    "Ydata = allVars[1]  #Input Data\n",
    "print(\"Ydata shape:\" + repr(Ydata.shape));\n",
    "print(\"Udata shape:\" + repr(Udata.shape));\n",
    "\n",
    "\n",
    "\n",
    "Ydata = np.asarray(Ydata,dtype=np.float32)\n",
    "\n",
    "Udata = np.asarray(Udata,dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.55108428e-01 1.33577541e-01 9.37128589e-02 5.84865846e-02\n",
      " 4.04109024e-02 2.22344398e-02 1.35624055e-02 1.11786025e-02\n",
      " 8.12430028e-03 6.66878512e-03 5.31834364e-03 5.13630547e-03\n",
      " 4.16420540e-03 3.88345076e-03 3.61132366e-03 3.09852092e-03\n",
      " 2.83042272e-03 2.62010517e-03 2.52207112e-03 2.09281128e-03\n",
      " 1.85531052e-03 1.64612487e-03 1.46547437e-03 1.33514893e-03\n",
      " 1.25007716e-03 1.05262059e-03 9.34131560e-04 8.97943100e-04\n",
      " 8.05314921e-04 7.25358317e-04 6.62337698e-04 5.69409342e-04\n",
      " 5.07836929e-04 4.85689554e-04 4.13584086e-04 3.94219591e-04\n",
      " 3.24366643e-04 3.19867017e-04 3.14850739e-04 2.83992209e-04\n",
      " 2.58426764e-04 2.40814625e-04 2.32569408e-04 2.12269384e-04\n",
      " 2.01877061e-04 1.88693579e-04 1.84825214e-04 1.67897524e-04\n",
      " 1.58146780e-04 1.53878937e-04]\n",
      "[680421.7   333776.66  279568.88  220860.16  183585.55  136176.55\n",
      " 106354.92   96556.83   82315.61   74578.32   66600.49   65450.754\n",
      "  58932.56   56911.25   54881.05   50835.42   48586.418  46746.445\n",
      "  45863.574  41778.637  39336.664  37052.766  34960.56   33369.836\n",
      "  32289.227  29629.557  27912.148  27366.146  25916.246  24596.059\n",
      "  23503.307  21792.223  20580.285  20126.516  18572.533  18132.527\n",
      "  16447.781  16333.301  16204.722  15390.136  14681.079  14171.984\n",
      "  13927.255  13305.553  12975.758  12544.917  12415.661  11833.449\n",
      "  11484.693  11328.667]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=50)\n",
    "pca.fit(Ydata)  \n",
    "PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
    "  svd_solver='auto', tol=0.0, whiten=False)\n",
    "print(pca.explained_variance_ratio_)  \n",
    "print(pca.singular_values_)  \n",
    "Ydata = pca.transform(Ydata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expose_deep_basis(z_list,num_bas_obs,deep_dict_size,iter_num,u):\n",
    "    basis_hooks = z_list[-1]; #[-1] is y  = K *\\phi; -2 is \\phi(yk)\n",
    "    x_range = np.arange(-10.0,10.0,0.1);\n",
    "\n",
    "    for i in range(0,num_bas_obs):\n",
    "        plt.close();\n",
    "        scan_injection = np.zeros((len(x_range),num_bas_obs));\n",
    "        scan_injection[:,i]= np.transpose(x_range);\n",
    "        phi_j = basis_hooks.eval(feed_dict={u:scan_injection});\n",
    "        fig_hand = plt.gcf()\n",
    "        plt.plot(x_range,phi_j,'.-',label='\\phi_i(y)');\n",
    "        #plt.ylim([-2.0,2.0]);\n",
    "        fig = plt.gcf();\n",
    "        plt.savefig('deep_basis_images/phi_with_u' + repr(i) + '_iternum_' + repr(iter_num) + '.jpg');\n",
    "\n",
    "    return fig_hand;        \n",
    "\n",
    "def xavier_init(n_inputs, n_outputs, uniform=True):\n",
    "    \"\"\"Set the parameter initialization using the method described.\n",
    "    This method is designed to keep the scale of the gradients roughly the same\n",
    "    in all layers.\n",
    "    Xavier Glorot and Yoshua Bengio (2010):\n",
    "       Understanding the difficulty of training deep feedforward neural\n",
    "       networks. International conference on artificial intelligence and\n",
    "       statistics.\n",
    "    Args:\n",
    "    n_inputs: The number of input nodes into each output.\n",
    "    n_outputs: The number of output nodes for each input.\n",
    "    uniform: If true use a uniform distribution, otherwise use a normal.\n",
    "    Returns:\n",
    "    An initializer.\n",
    "    \"\"\"\n",
    "    if uniform:\n",
    "        # 6 was used in the paper.\n",
    "        init_range = math.sqrt(6.0 / (n_inputs + n_outputs))\n",
    "        return tf.random_uniform_initializer(-init_range, init_range)\n",
    "    else:\n",
    "        # 3 gives us approximately the same limits as above since this repicks\n",
    "        # values greater than 2 standard deviations from the mean.\n",
    "        stddev = math.sqrt(3.0 / (n_inputs + n_outputs))\n",
    "        return tf.truncated_normal_initializer(stddev=stddev)\n",
    "\n",
    "def weight_variable(shape):\n",
    "    std_dev = math.sqrt(3.0 /(shape[0] + shape[1]))\n",
    "    return tf.Variable(tf.truncated_normal(shape, mean=0.0,stddev=std_dev,dtype=tf.float32));\n",
    "  \n",
    "def bias_variable(shape):\n",
    "    std_dev = math.sqrt(3.0 / shape[0])\n",
    "    return tf.Variable(tf.truncated_normal(shape, mean=0.0,stddev=std_dev,dtype=tf.float32));\n",
    "\n",
    "\n",
    "def network_assemble(input_var,W_list,b_list,keep_prob=1.0,activation_flag=1,res_net=0):\n",
    "    n_depth = len(W_list);\n",
    "    print(\"n_depth: \" + repr(n_depth));\n",
    "    z_temp_list = [];\n",
    "    \n",
    "    for k in range(0,n_depth):\n",
    "        \n",
    "        if (k==0):\n",
    "            W1 = W_list[0];\n",
    "            b1 = b_list[0];\n",
    "            if activation_flag==1:# RELU\n",
    "                z1 = tf.nn.dropout(tf.nn.relu(tf.matmul(input_var,W1)+b1),keep_prob);\n",
    "            if activation_flag==2: #ELU \n",
    "                z1 = tf.nn.dropout(tf.nn.elu(tf.matmul(input_var,W1)+b1),keep_prob);\n",
    "            if activation_flag==3: # tanh\n",
    "                z1 = tf.nn.dropout(tf.nn.tanh(tf.matmul(input_var,W1)+b1),keep_prob);\n",
    "\n",
    "            z_temp_list.append(z1);\n",
    "            \n",
    "\n",
    "        if not (k==0) and k < (n_depth-1):\n",
    "            \n",
    "            prev_layer_output = tf.matmul(z_temp_list[k-1],W_list[k])+b_list[k]\n",
    "\n",
    "            if res_net and k==(n_depth-2):\n",
    "                prev_layer_output += tf.matmul(u,W1)+b1 #  this expression is not compatible for variable width nets (where each layer has a different width at inialization - okay with regularization and dropout afterwards though)\n",
    "\n",
    "            if activation_flag==1:\n",
    "                z_temp_list.append(tf.nn.dropout(tf.nn.relu(prev_layer_output),keep_prob));\n",
    "            if activation_flag==2:\n",
    "                z_temp_list.append(tf.nn.dropout(tf.nn.elu(prev_layer_output),keep_prob));\n",
    "            if activation_flag==3:\n",
    "                z_temp_list.append(tf.nn.dropout(tf.nn.tanh(prev_layer_output),keep_prob));\n",
    "\n",
    "                \n",
    "        if not (k==0) and k == (n_depth-1):\n",
    "            prev_layer_output = tf.matmul(z_temp_list[k-1],W_list[k])+b_list[k];\n",
    "            z_temp_list.append(prev_layer_output);\n",
    "\n",
    "        \n",
    "    if debug_splash:\n",
    "        print(\"[DEBUG] z_list\" + repr(z_list[-1]));\n",
    "        \n",
    "    #y_out = tf.concat([z_list[-1],u],axis=1); # last element of activation output list is the actual NN output\n",
    "    y_out = z_temp_list[-1];\n",
    "    \n",
    "    result = sess.run(tf.global_variables_initializer())\n",
    "    return y_out,z_temp_list;\n",
    "\n",
    "\n",
    "def initialize_Wblist(n_u,hv_list):\n",
    "    W_list = [];\n",
    "    b_list = [];\n",
    "    n_depth = len(hv_list);\n",
    "    print(\"Length of hv_list: \" + repr(n_depth))\n",
    "    #hv_list[n_depth-1] = n_y;\n",
    "    for k in range(0,n_depth):\n",
    "        \n",
    "        if k==0:\n",
    "            W1 = weight_variable([n_u,hv_list[k]]);\n",
    "            b1 = bias_variable([hv_list[k]]);\n",
    "            W_list.append(W1);\n",
    "            b_list.append(b1);\n",
    "        else:\n",
    "            W_list.append(weight_variable([hv_list[k-1],hv_list[k]]));\n",
    "            b_list.append(bias_variable([hv_list[k]]));\n",
    "    result = sess.run(tf.global_variables_initializer())\n",
    "    return W_list,b_list;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(y_all_training,y_feed,u_all_training,u_feed,obj_func,optimizer,u_control_all_training=None,valid_error_thres=1e-2,test_error_thres=1e-2,max_iters=100000,step_size_val=0.01,batchsize=10):\n",
    "\n",
    "  iter = 0;\n",
    "  samplerate = 5000;\n",
    "  good_start = 1;\n",
    "  valid_error = 100.0;\n",
    "  test_error = 100.0;\n",
    "  training_error_history_nocovar = [];\n",
    "  validation_error_history_nocovar = [];\n",
    "  test_error_history_nocovar = [];\n",
    "\n",
    "  training_error_history_withcovar = [];\n",
    "  validation_error_history_withcovar = [];\n",
    "  test_error_history_withcovar = [];\n",
    "\n",
    "\n",
    "  while (((test_error>test_error_thres) or (valid_error > valid_error_thres)) and iter < max_iters):\n",
    "    iter+=1;\n",
    "    \n",
    "    all_ind = set(np.arange(0,len(u_all_training)));\n",
    "    select_ind = np.random.randint(0,len(u_all_training),size=batchsize);\n",
    "    valid_ind = list(all_ind -set(select_ind))[0:batchsize];\n",
    "    select_ind_test = list(all_ind - set(valid_ind) - set(select_ind))[0:batchsize];\n",
    "\n",
    "    \n",
    "    u_batch =[];\n",
    "    u_control_batch = [];\n",
    "    y_batch = [];\n",
    "    u_valid = [];\n",
    "    u_control_valid = [];\n",
    "    y_valid = [];\n",
    "    u_test_train = [];\n",
    "    u_control_train = [];\n",
    "    y_test_train= [];\n",
    "    u_control_test_train = [];\n",
    "    \n",
    "    for j in range(0,len(select_ind)):\n",
    "      u_batch.append(u_all_training[select_ind[j]]);  \n",
    "#      y_batch = embed_feed.eval(feed_dict={u_feed:u_batch});\n",
    "      y_batch.append(y_all_training[select_ind[j]]);\n",
    "          \n",
    "    for k in range(0,len(valid_ind)):\n",
    "      u_valid.append(u_all_training[valid_ind[k]]);\n",
    "      y_valid.append(y_all_training[valid_ind[k]]);\n",
    "#      y_valid = embed_feed.eval(feed_dict={u_feed:u_valid});\n",
    "\n",
    "\n",
    "    for k in range(0,len(select_ind_test)):\n",
    "      u_test_train.append(u_all_training[select_ind_test[k]]);\n",
    "      y_test_train.append(y_all_training[select_ind_test[k]]);  \n",
    "#    y_test_train = embed_feed.eval(feed_dict={u_feed:u_test_train});\n",
    "    u_batch = np.asarray(u_batch);\n",
    "    y_batch = np.asarray(y_batch);\n",
    "    \n",
    "    optimizer.run(feed_dict={u_feed:u_batch,y_feed:y_batch,step_size:step_size_val});\n",
    "    valid_error = obj_func.eval(feed_dict={u_feed:u_valid,y_feed:y_valid});\n",
    "    test_error = obj_func.eval(feed_dict={u_feed:u_test_train,y_feed:y_test_train});\n",
    "\n",
    "    \n",
    "    if iter%samplerate==0:\n",
    "      training_error_history_nocovar.append(obj_func.eval(feed_dict={u_feed:u_batch,y_feed:y_batch}));\n",
    "      validation_error_history_nocovar.append(obj_func.eval(feed_dict={u_feed:u_valid,y_feed:y_valid}));\n",
    "      test_error_history_nocovar.append(obj_func.eval(feed_dict={u_feed:u_test_train,y_feed:y_test_train}));\n",
    "\n",
    "  \n",
    "      if (iter%10==0) or (iter==1):\n",
    "        #plt.close();                  \n",
    "        print (\"step %d , validation error %g\"%(iter, obj_func.eval(feed_dict={u_feed:u_valid,y_feed:y_valid})));\n",
    "        print (\"step %d , test error %g\"%(iter, obj_func.eval(feed_dict={u_feed:u_test_train,y_feed:y_test_train})));\n",
    "        #print(\"Reconstruction Loss: \" + repr(this_vae_loss.eval(feed_dict={this_u:this_corpus_vec})))\n",
    "#        this_corpus_embed = embed_feed.eval(feed_dict={u_feed:})\n",
    "        #print(\"Embedding Loss: \" + repr(this_embed_loss.eval(feed_dict={this_u:this_corpus_vec})) )\n",
    "    \n",
    "#    if ((iter>20000) and iter%10) :#\n",
    "#\n",
    "#      valid_gradient = np.gradient(np.asarray(validation_error_history_nocovar[iter/samplerate*7/10:]));\n",
    "#      mu_gradient = np.mean(valid_gradient);\n",
    "#\n",
    "#      if ((iter <1000) and (mu_gradient >= 5e-1)): # eventually update this to be 1/10th the mean of batch data, or mean of all data handed as input param to func\n",
    "#        good_start = 0; # if after 10,000 iterations validation error is still above 1e0, initialization was poor.\n",
    "#        print(\"Terminating model refinement loop with gradient:\") + repr(mu_gradient) + \", validation error after \" + repr(iter) + \" epochs:  \" + repr(valid_error);\n",
    "#        iter = max_iters; # terminate while loop and return histories\n",
    "\n",
    "  all_histories = [training_error_history_nocovar, validation_error_history_nocovar,test_error_history_nocovar];\n",
    "  \n",
    "  plt.close();\n",
    "  x = np.arange(0,len(validation_error_history_nocovar),1);\n",
    "  plt.plot(x,training_error_history_nocovar,label='train. err.');\n",
    "  plt.plot(x,validation_error_history_nocovar,label='valid. err.');\n",
    "  plt.plot(x,test_error_history_nocovar,label='test err.');\n",
    "  #plt.gca().set_yscale('log');\n",
    "  plt.savefig('all_error_history.pdf');\n",
    "  \n",
    "  plt.close();\n",
    "  return all_histories,good_start;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def vae_loss(y_model,y_true):\n",
    "        return tf.norm(y_true - y_model,axis=[0,1],ord='fro')#/tf.norm(y_true,axis=[0,1],ord='fro');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim_parameter = 2; \n",
    "#label_dim = 1; \n",
    "intermediate_dim = 100\n",
    "output_dim = Ydata.shape[1];\n",
    "batch_size_parameter=200;#4000 for howard's e. coli dataset\n",
    "debug_splash = 0;\n",
    "this_step_size_val = 0.025;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of hv_list: 3\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "n_depth: 3\n",
      "WARNING:tensorflow:From <ipython-input-4-51c51ed58bea>:65: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "step 5000 , validation error 593976\n",
      "step 5000 , test error 488555\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-fb1a745e5ff0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#this_embed_loss = embed_loss(this_u,this_embedding);\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtrain_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthis_y_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mUdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthis_u\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthis_io_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthis_optim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatchsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size_parameter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep_size_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthis_step_size_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-24d3a76e541f>\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(y_all_training, y_feed, u_all_training, u_feed, obj_func, optimizer, u_control_all_training, valid_error_thres, test_error_thres, max_iters, step_size_val, batchsize)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_error\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mtest_error_thres\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalid_error\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mvalid_error_thres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0miter\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_vars_list = [input_dim_parameter,intermediate_dim,output_dim];\n",
    "\n",
    "this_u = tf.placeholder(tf.float32, shape=[None,input_dim_parameter],name=\"InputInducers\");\n",
    "with tf.device('/cpu:0'):\n",
    "    this_W_list,this_b_list = initialize_Wblist(input_dim_parameter,hidden_vars_list);\n",
    "    this_y_out,all_layers = network_assemble(this_u,this_W_list,this_b_list,keep_prob=1.0,activation_flag=2,res_net=0)\n",
    "\n",
    "    this_y_true = tf.placeholder(tf.float32,shape=[None,output_dim],name=\"Groundtruth_Transcriptome\")    \n",
    "    this_output_layer = all_layers[-3]\n",
    "\n",
    "    result = sess.run(tf.global_variables_initializer())\n",
    "    this_io_loss = vae_loss(this_y_out,this_y_true)\n",
    "    this_optim = tf.train.AdagradOptimizer(learning_rate=this_step_size_val).minimize(this_io_loss)\n",
    "    step_size = tf.placeholder(tf.float32,shape=[],name=\"StepSizePlaceholder\");\n",
    "    result = sess.run(tf.global_variables_initializer())\n",
    "    #this_embed_loss = embed_loss(this_u,this_embedding);\n",
    "\n",
    "    train_net(Ydata,this_y_true,Udata,this_u,this_io_loss,this_optim,batchsize = batch_size_parameter,step_size_val = this_step_size_val,max_iters=5e4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YT*U*(U^T*U)^{-1} = G "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = np.matmul(Ydata.T,np.matmul(Udata,np.linalg.inv(np.matmul(Udata.T,Udata )) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "\n",
    "this_xlabels = ['arabinose','iptg']\n",
    "#this_ylabels = ['gene'+repr(ind) for ind in range(0,4110)]\n",
    "sns.set(font_scale=1.4)\n",
    "#hm=sns.heatmap(G,xticklabels=this_xlabels,#yticklabels=this_ylabels,cmap='RdYlGn',annot=False)\n",
    "hm=sns.heatmap(G,xticklabels=this_xlabels,cmap='RdYlGn',annot=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
