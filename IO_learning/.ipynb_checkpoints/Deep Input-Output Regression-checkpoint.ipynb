{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named google.protobuf",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9526b75778b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Protocol buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_def_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdescriptor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_descriptor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreflection\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_reflection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named google.protobuf"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np;\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math;\n",
    "import random;\n",
    "import matplotlib.pyplot as plt;\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_obj = open('NAND_Titration_IO_data.pickle','rb')\n",
    "allVars = pickle.load(file_obj)\n",
    " = allVars[0]\n",
    "    = allVars[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expose_deep_basis(z_list,num_bas_obs,deep_dict_size,iter_num,u):\n",
    "    basis_hooks = z_list[-1]; #[-1] is y  = K *\\phi; -2 is \\phi(yk)\n",
    "    x_range = np.arange(-10.0,10.0,0.1);\n",
    "\n",
    "    for i in range(0,num_bas_obs):\n",
    "        plt.close();\n",
    "        scan_injection = np.zeros((len(x_range),num_bas_obs));\n",
    "        scan_injection[:,i]= np.transpose(x_range);\n",
    "        phi_j = basis_hooks.eval(feed_dict={u:scan_injection});\n",
    "        fig_hand = plt.gcf()\n",
    "        plt.plot(x_range,phi_j,'.-',label='\\phi_i(y)');\n",
    "        #plt.ylim([-2.0,2.0]);\n",
    "        fig = plt.gcf();\n",
    "        plt.savefig('deep_basis_images/phi_with_u' + repr(i) + '_iternum_' + repr(iter_num) + '.jpg');\n",
    "\n",
    "    return fig_hand;        \n",
    "\n",
    "def xavier_init(n_inputs, n_outputs, uniform=True):\n",
    "    \"\"\"Set the parameter initialization using the method described.\n",
    "    This method is designed to keep the scale of the gradients roughly the same\n",
    "    in all layers.\n",
    "    Xavier Glorot and Yoshua Bengio (2010):\n",
    "       Understanding the difficulty of training deep feedforward neural\n",
    "       networks. International conference on artificial intelligence and\n",
    "       statistics.\n",
    "    Args:\n",
    "    n_inputs: The number of input nodes into each output.\n",
    "    n_outputs: The number of output nodes for each input.\n",
    "    uniform: If true use a uniform distribution, otherwise use a normal.\n",
    "    Returns:\n",
    "    An initializer.\n",
    "    \"\"\"\n",
    "    if uniform:\n",
    "        # 6 was used in the paper.\n",
    "        init_range = math.sqrt(6.0 / (n_inputs + n_outputs))\n",
    "        return tf.random_uniform_initializer(-init_range, init_range)\n",
    "    else:\n",
    "        # 3 gives us approximately the same limits as above since this repicks\n",
    "        # values greater than 2 standard deviations from the mean.\n",
    "        stddev = math.sqrt(3.0 / (n_inputs + n_outputs))\n",
    "        return tf.truncated_normal_initializer(stddev=stddev)\n",
    "\n",
    "def weight_variable(shape):\n",
    "    std_dev = math.sqrt(3.0 /(shape[0] + shape[1]))\n",
    "    return tf.Variable(tf.truncated_normal(shape, mean=0.0,stddev=std_dev,dtype=tf.float32));\n",
    "  \n",
    "def bias_variable(shape):\n",
    "    std_dev = math.sqrt(3.0 / shape[0])\n",
    "    return tf.Variable(tf.truncated_normal(shape, mean=0.0,stddev=std_dev,dtype=tf.float32));\n",
    "\n",
    "\n",
    "def network_assemble(input_var,W_list,b_list,keep_prob=1.0,activation_flag=1,res_net=0):\n",
    "    n_depth = len(W_list);\n",
    "    print(\"n_depth: \" + repr(n_depth));\n",
    "    z_temp_list = [];\n",
    "    \n",
    "    for k in range(0,n_depth):\n",
    "        \n",
    "        if (k==0):\n",
    "            W1 = W_list[0];\n",
    "            b1 = b_list[0];\n",
    "            if activation_flag==1:# RELU\n",
    "                z1 = tf.nn.dropout(tf.nn.relu(tf.matmul(input_var,W1)+b1),keep_prob);\n",
    "            if activation_flag==2: #ELU \n",
    "                z1 = tf.nn.dropout(tf.nn.elu(tf.matmul(input_var,W1)+b1),keep_prob);\n",
    "            if activation_flag==3: # tanh\n",
    "                z1 = tf.nn.dropout(tf.nn.tanh(tf.matmul(input_var,W1)+b1),keep_prob);\n",
    "\n",
    "            z_temp_list.append(z1);\n",
    "            \n",
    "\n",
    "        if not (k==0) and k < (n_depth-1):\n",
    "            \n",
    "            prev_layer_output = tf.matmul(z_temp_list[k-1],W_list[k])+b_list[k]\n",
    "\n",
    "            if res_net and k==(n_depth-2):\n",
    "                prev_layer_output += tf.matmul(u,W1)+b1 #  this expression is not compatible for variable width nets (where each layer has a different width at inialization - okay with regularization and dropout afterwards though)\n",
    "\n",
    "            if activation_flag==1:\n",
    "                z_temp_list.append(tf.nn.dropout(tf.nn.relu(prev_layer_output),keep_prob));\n",
    "            if activation_flag==2:\n",
    "                z_temp_list.append(tf.nn.dropout(tf.nn.elu(prev_layer_output),keep_prob));\n",
    "            if activation_flag==3:\n",
    "                z_temp_list.append(tf.nn.dropout(tf.nn.tanh(prev_layer_output),keep_prob));\n",
    "\n",
    "                \n",
    "        if not (k==0) and k == (n_depth-1):\n",
    "            prev_layer_output = tf.matmul(z_temp_list[k-1],W_list[k])+b_list[k];\n",
    "            z_temp_list.append(prev_layer_output);\n",
    "\n",
    "        \n",
    "    if debug_splash:\n",
    "        print(\"[DEBUG] z_list\" + repr(z_list[-1]));\n",
    "        \n",
    "    #y_out = tf.concat([z_list[-1],u],axis=1); # last element of activation output list is the actual NN output\n",
    "    y_out = z_temp_list[-1];\n",
    "    \n",
    "    result = sess.run(tf.global_variables_initializer())\n",
    "    return y_out,z_temp_list;\n",
    "\n",
    "\n",
    "def initialize_Wblist(n_u,hv_list):\n",
    "    W_list = [];\n",
    "    b_list = [];\n",
    "    n_depth = len(hv_list);\n",
    "    print(\"Length of hv_list: \" + repr(n_depth))\n",
    "    #hv_list[n_depth-1] = n_y;\n",
    "    for k in range(0,n_depth):\n",
    "        \n",
    "        if k==0:\n",
    "            W1 = weight_variable([n_u,hv_list[k]]);\n",
    "            b1 = bias_variable([hv_list[k]]);\n",
    "            W_list.append(W1);\n",
    "            b_list.append(b1);\n",
    "        else:\n",
    "            W_list.append(weight_variable([hv_list[k-1],hv_list[k]]));\n",
    "            b_list.append(bias_variable([hv_list[k]]));\n",
    "    result = sess.run(tf.global_variables_initializer())\n",
    "    return W_list,b_list;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(u_all_training,u_feed,obj_func,optimizer,u_control_all_training=None,valid_error_thres=1e-2,test_error_thres=1e-2,max_iters=100000,step_size_val=0.01,batchsize=10):\n",
    "\n",
    "  iter = 0;\n",
    "  samplerate = 5000;\n",
    "  good_start = 1;\n",
    "  valid_error = 100.0;\n",
    "  test_error = 100.0;\n",
    "  training_error_history_nocovar = [];\n",
    "  validation_error_history_nocovar = [];\n",
    "  test_error_history_nocovar = [];\n",
    "\n",
    "  training_error_history_withcovar = [];\n",
    "  validation_error_history_withcovar = [];\n",
    "  test_error_history_withcovar = [];\n",
    "\n",
    "\n",
    "  while (((test_error>test_error_thres) or (valid_error > valid_error_thres)) and iter < max_iters):\n",
    "    iter+=1;\n",
    "    \n",
    "    all_ind = set(np.arange(0,len(u_all_training)));\n",
    "    select_ind = np.random.randint(0,len(u_all_training),size=batchsize);\n",
    "    valid_ind = list(all_ind -set(select_ind))[0:batchsize];\n",
    "    select_ind_test = list(all_ind - set(valid_ind) - set(select_ind))[0:batchsize];\n",
    "\n",
    "    \n",
    "    u_batch =[];\n",
    "    u_control_batch = [];\n",
    "#    y_batch = [];\n",
    "    u_valid = [];\n",
    "    u_control_valid = [];\n",
    "#    y_valid = [];\n",
    "    u_test_train = [];\n",
    "    u_control_train = [];\n",
    "#    y_test_train= [];\n",
    "    u_control_test_train = [];\n",
    "    \n",
    "    for j in range(0,len(select_ind)):\n",
    "      u_batch.append(u_all_training[select_ind[j]]);\n",
    "    \n",
    "#    y_batch = embed_feed.eval(feed_dict={u_feed:u_batch});\n",
    "    \n",
    "#      y_batch.append(y_all_training[select_ind[j]]);\n",
    "          \n",
    "    for k in range(0,len(valid_ind)):\n",
    "      u_valid.append(u_all_training[valid_ind[k]]);\n",
    "\n",
    "#    y_valid = embed_feed.eval(feed_dict={u_feed:u_valid});\n",
    "\n",
    "\n",
    "    for k in range(0,len(select_ind_test)):\n",
    "      u_test_train.append(u_all_training[select_ind_test[k]]);\n",
    "\n",
    "#    y_test_train = embed_feed.eval(feed_dict={u_feed:u_test_train});\n",
    "\n",
    "\n",
    "    optimizer.run(feed_dict={u_feed:u_batch})#,embed_feed:,step_size:step_size_val});\n",
    "    valid_error = obj_func.eval(feed_dict={u_feed:u_valid})#,embed_feed:y_valid});\n",
    "    test_error = obj_func.eval(feed_dict={u_feed:u_test_train})#,embed_feed:y_test_train});\n",
    "\n",
    "    \n",
    "    if iter%samplerate==0:\n",
    "      training_error_history_nocovar.append(obj_func.eval(feed_dict={u_feed:u_batch}))#,embed_feed:y_batch}));\n",
    "      validation_error_history_nocovar.append(obj_func.eval(feed_dict={u_feed:u_valid}))#,embed_feed:y_valid}));\n",
    "      test_error_history_nocovar.append(obj_func.eval(feed_dict={u_feed:u_test_train}))#,embed_feed:y_test_train}));\n",
    "\n",
    "  \n",
    "      if (iter%10==0) or (iter==1):\n",
    "        #plt.close();                  \n",
    "        print (\"step %d , validation error %g\"%(iter, obj_func.eval(feed_dict={u_feed:u_valid})))#,embed_feed:y_valid})));\n",
    "        print (\"step %d , test error %g\"%(iter, obj_func.eval(feed_dict={u_feed:u_test_train})));#,embed_feed:y_test_train})));\n",
    "        print(\"Reconstruction Loss: \" + repr(this_vae_loss.eval(feed_dict={this_u:this_corpus_vec})))\n",
    "#        this_corpus_embed = embed_feed.eval(feed_dict={u_feed:})\n",
    "        #print(\"Embedding Loss: \" + repr(this_embed_loss.eval(feed_dict={this_u:this_corpus_vec})) )\n",
    "    \n",
    "#    if ((iter>20000) and iter%10) :#\n",
    "#\n",
    "#      valid_gradient = np.gradient(np.asarray(validation_error_history_nocovar[iter/samplerate*7/10:]));\n",
    "#      mu_gradient = np.mean(valid_gradient);\n",
    "#\n",
    "#      if ((iter <1000) and (mu_gradient >= 5e-1)): # eventually update this to be 1/10th the mean of batch data, or mean of all data handed as input param to func\n",
    "#        good_start = 0; # if after 10,000 iterations validation error is still above 1e0, initialization was poor.\n",
    "#        print(\"Terminating model refinement loop with gradient:\") + repr(mu_gradient) + \", validation error after \" + repr(iter) + \" epochs:  \" + repr(valid_error);\n",
    "#        iter = max_iters; # terminate while loop and return histories\n",
    "\n",
    "  all_histories = [training_error_history_nocovar, validation_error_history_nocovar,test_error_history_nocovar];\n",
    "  \n",
    "  plt.close();\n",
    "  x = np.arange(0,len(validation_error_history_nocovar),1);\n",
    "  plt.plot(x,training_error_history_nocovar,label='train. err.');\n",
    "  plt.plot(x,validation_error_history_nocovar,label='valid. err.');\n",
    "  plt.plot(x,test_error_history_nocovar,label='test err.');\n",
    "  #plt.gca().set_yscale('log');\n",
    "  plt.savefig('all_error_history.pdf');\n",
    "  \n",
    "  plt.close();\n",
    "  return all_histories,good_start;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(y_model,y_true):\n",
    "    return tf.norm(y_true - y_model,axis=[0,1],ord=2)/tf.norm(y_true,axis=[0,1],ord=2);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim_parameter = 2; \n",
    "#label_dim = 1; \n",
    "intermediate_dim = 400\n",
    "output_dim = 4095;\n",
    "batch_size_parameter=30;#4000 for howard's e. coli dataset\n",
    "debug_splash = 0;\n",
    "this_step_size_val = 0.025;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of hv_list: 4\n",
      "n_depth: 4\n",
      "WARNING:tensorflow:From <ipython-input-3-51c51ed58bea>:65: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "hidden_vars_list = [output_dim,intermediate_dim,intermediate_dim,input_dim_parameter];\n",
    "\n",
    "this_u = tf.placeholder(tf.float32, shape=[None,input_dim_parameter]);\n",
    "with tf.device('/cpu:0'):\n",
    "    this_W_list,this_b_list = initialize_Wblist(input_dim_parameter,hidden_vars_list);\n",
    "    this_y_out,all_layers = network_assemble(this_u,this_W_list,this_b_list,keep_prob=1.0,activation_flag=2,res_net=0)\n",
    "\n",
    "this_y_true = tf.placeholder(tf.float32,shape=[None,output_dim])    \n",
    "this_output_layer = all_layers[-3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_OutputLoss = vae_loss(this_y_out,this_y_true);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
