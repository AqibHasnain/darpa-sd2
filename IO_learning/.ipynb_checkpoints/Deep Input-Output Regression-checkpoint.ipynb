{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np;\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math;\n",
    "import random;\n",
    "import matplotlib.pyplot as plt;\n",
    "import pickle\n",
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ydata shape:(960, 4110)\n",
      "Udata shape:(960, 2)\n"
     ]
    }
   ],
   "source": [
    "file_obj = open('NAND_Titration_IO_data.pickle','rb')\n",
    "allVars = pickle.load(file_obj)\n",
    "Udata = allVars[0]  #Output Data \n",
    "Ydata = allVars[1]  #Input Data\n",
    "print(\"Ydata shape:\" + repr(Ydata.shape));\n",
    "print(\"Udata shape:\" + repr(Udata.shape));\n",
    "\n",
    "\n",
    "\n",
    "Ydata = np.asarray(Ydata,dtype=np.float32)\n",
    "\n",
    "Udata = np.asarray(Udata,dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.9277259e-01 1.3513850e-01 1.0089717e-01 8.0459222e-02 6.5239623e-02\n",
      " 4.5967609e-02 3.1461325e-02 2.9222982e-02 1.9508241e-02 1.2258849e-02\n",
      " 9.7580235e-03 8.4545463e-03 7.8107538e-03 5.8577638e-03 5.1337997e-03\n",
      " 4.5277411e-03 3.4203660e-03 3.0320219e-03 2.9579045e-03 2.6296913e-03\n",
      " 2.2058736e-03 2.0511991e-03 2.0339440e-03 1.8666225e-03 1.6641719e-03\n",
      " 1.4426636e-03 1.3382123e-03 1.2992744e-03 1.1056926e-03 1.0642762e-03\n",
      " 1.0305539e-03 9.3600963e-04 9.0183923e-04 8.9412893e-04 8.3654170e-04\n",
      " 7.8847510e-04 7.3956687e-04 6.7990378e-04 5.6674087e-04 5.2328885e-04\n",
      " 4.7962402e-04 4.7443929e-04 4.3330525e-04 4.2307697e-04 3.8005420e-04\n",
      " 3.7418341e-04 3.5297830e-04 3.3045441e-04 3.0140585e-04 2.8095749e-04]\n",
      "[880463.7   516452.53  446252.2   398500.4   358836.47  301208.22\n",
      " 249189.28  240161.36  196223.    155548.45  138778.38  129177.25\n",
      " 124161.61  107524.34  100660.79   94532.61   82163.164  77358.32\n",
      "  76406.96   72043.24   65982.87   63627.484  63359.297  60697.26\n",
      "  57311.26   53360.93   51392.93   50639.723  46715.184  45831.918\n",
      "  45099.97   42981.445  42189.6    42008.863  40633.54   39448.895\n",
      "  38205.824  36632.332  33445.15   32137.465  30767.438  30600.688\n",
      "  29244.07   28896.854  27388.209  27175.85   26394.586  25538.572\n",
      "  24390.275  23548.387]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=50)\n",
    "pca.fit(Ydata)  \n",
    "PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
    "  svd_solver='auto', tol=0.0, whiten=False)\n",
    "print(pca.explained_variance_ratio_)  \n",
    "print(pca.singular_values_)  \n",
    "Ydata = pca.transform(Ydata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expose_deep_basis(z_list,num_bas_obs,deep_dict_size,iter_num,u):\n",
    "    basis_hooks = z_list[-1]; #[-1] is y  = K *\\phi; -2 is \\phi(yk)\n",
    "    x_range = np.arange(-10.0,10.0,0.1);\n",
    "\n",
    "    for i in range(0,num_bas_obs):\n",
    "        plt.close();\n",
    "        scan_injection = np.zeros((len(x_range),num_bas_obs));\n",
    "        scan_injection[:,i]= np.transpose(x_range);\n",
    "        phi_j = basis_hooks.eval(feed_dict={u:scan_injection});\n",
    "        fig_hand = plt.gcf()\n",
    "        plt.plot(x_range,phi_j,'.-',label='\\phi_i(y)');\n",
    "        #plt.ylim([-2.0,2.0]);\n",
    "        fig = plt.gcf();\n",
    "        plt.savefig('deep_basis_images/phi_with_u' + repr(i) + '_iternum_' + repr(iter_num) + '.jpg');\n",
    "\n",
    "    return fig_hand;        \n",
    "\n",
    "def xavier_init(n_inputs, n_outputs, uniform=True):\n",
    "    \"\"\"Set the parameter initialization using the method described.\n",
    "    This method is designed to keep the scale of the gradients roughly the same\n",
    "    in all layers.\n",
    "    Xavier Glorot and Yoshua Bengio (2010):\n",
    "       Understanding the difficulty of training deep feedforward neural\n",
    "       networks. International conference on artificial intelligence and\n",
    "       statistics.\n",
    "    Args:\n",
    "    n_inputs: The number of input nodes into each output.\n",
    "    n_outputs: The number of output nodes for each input.\n",
    "    uniform: If true use a uniform distribution, otherwise use a normal.\n",
    "    Returns:\n",
    "    An initializer.\n",
    "    \"\"\"\n",
    "    if uniform:\n",
    "        # 6 was used in the paper.\n",
    "        init_range = math.sqrt(6.0 / (n_inputs + n_outputs))\n",
    "        return tf.random_uniform_initializer(-init_range, init_range)\n",
    "    else:\n",
    "        # 3 gives us approximately the same limits as above since this repicks\n",
    "        # values greater than 2 standard deviations from the mean.\n",
    "        stddev = math.sqrt(3.0 / (n_inputs + n_outputs))\n",
    "        return tf.truncated_normal_initializer(stddev=stddev)\n",
    "\n",
    "def weight_variable(shape):\n",
    "    std_dev = math.sqrt(3.0 /(shape[0] + shape[1]))\n",
    "    return tf.Variable(tf.truncated_normal(shape, mean=0.0,stddev=std_dev,dtype=tf.float32));\n",
    "  \n",
    "def bias_variable(shape):\n",
    "    std_dev = math.sqrt(3.0 / shape[0])\n",
    "    return tf.Variable(tf.truncated_normal(shape, mean=0.0,stddev=std_dev,dtype=tf.float32));\n",
    "\n",
    "\n",
    "def network_assemble(input_var,W_list,b_list,keep_prob=1.0,activation_flag=1,res_net=0):\n",
    "    n_depth = len(W_list);\n",
    "    print(\"n_depth: \" + repr(n_depth));\n",
    "    z_temp_list = [];\n",
    "    \n",
    "    for k in range(0,n_depth):\n",
    "        \n",
    "        if (k==0):\n",
    "            W1 = W_list[0];\n",
    "            b1 = b_list[0];\n",
    "            if activation_flag==1:# RELU\n",
    "                z1 = tf.nn.dropout(tf.nn.relu(tf.matmul(input_var,W1)+b1),keep_prob);\n",
    "            if activation_flag==2: #ELU \n",
    "                z1 = tf.nn.dropout(tf.nn.elu(tf.matmul(input_var,W1)+b1),keep_prob);\n",
    "            if activation_flag==3: # tanh\n",
    "                z1 = tf.nn.dropout(tf.nn.tanh(tf.matmul(input_var,W1)+b1),keep_prob);\n",
    "\n",
    "            z_temp_list.append(z1);\n",
    "            \n",
    "\n",
    "        if not (k==0) and k < (n_depth-1):\n",
    "            \n",
    "            prev_layer_output = tf.matmul(z_temp_list[k-1],W_list[k])+b_list[k]\n",
    "\n",
    "            if res_net and k==(n_depth-2):\n",
    "                prev_layer_output += tf.matmul(u,W1)+b1 #  this expression is not compatible for variable width nets (where each layer has a different width at inialization - okay with regularization and dropout afterwards though)\n",
    "\n",
    "            if activation_flag==1:\n",
    "                z_temp_list.append(tf.nn.dropout(tf.nn.relu(prev_layer_output),keep_prob));\n",
    "            if activation_flag==2:\n",
    "                z_temp_list.append(tf.nn.dropout(tf.nn.elu(prev_layer_output),keep_prob));\n",
    "            if activation_flag==3:\n",
    "                z_temp_list.append(tf.nn.dropout(tf.nn.tanh(prev_layer_output),keep_prob));\n",
    "\n",
    "                \n",
    "        if not (k==0) and k == (n_depth-1):\n",
    "            prev_layer_output = tf.matmul(z_temp_list[k-1],W_list[k])+b_list[k];\n",
    "            z_temp_list.append(prev_layer_output);\n",
    "\n",
    "        \n",
    "    if debug_splash:\n",
    "        print(\"[DEBUG] z_list\" + repr(z_list[-1]));\n",
    "        \n",
    "    #y_out = tf.concat([z_list[-1],u],axis=1); # last element of activation output list is the actual NN output\n",
    "    y_out = z_temp_list[-1];\n",
    "    \n",
    "    result = sess.run(tf.global_variables_initializer())\n",
    "    return y_out,z_temp_list;\n",
    "\n",
    "\n",
    "def initialize_Wblist(n_u,hv_list):\n",
    "    W_list = [];\n",
    "    b_list = [];\n",
    "    n_depth = len(hv_list);\n",
    "    print(\"Length of hv_list: \" + repr(n_depth))\n",
    "    #hv_list[n_depth-1] = n_y;\n",
    "    for k in range(0,n_depth):\n",
    "        \n",
    "        if k==0:\n",
    "            W1 = weight_variable([n_u,hv_list[k]]);\n",
    "            b1 = bias_variable([hv_list[k]]);\n",
    "            W_list.append(W1);\n",
    "            b_list.append(b1);\n",
    "        else:\n",
    "            W_list.append(weight_variable([hv_list[k-1],hv_list[k]]));\n",
    "            b_list.append(bias_variable([hv_list[k]]));\n",
    "    result = sess.run(tf.global_variables_initializer())\n",
    "    return W_list,b_list;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(y_all_training,y_feed,u_all_training,u_feed,obj_func,optimizer,u_control_all_training=None,valid_error_thres=1e-2,test_error_thres=1e-2,max_iters=100000,step_size_val=0.01,batchsize=10):\n",
    "\n",
    "  iter = 0;\n",
    "  samplerate = 5000;\n",
    "  good_start = 1;\n",
    "  valid_error = 100.0;\n",
    "  test_error = 100.0;\n",
    "  training_error_history_nocovar = [];\n",
    "  validation_error_history_nocovar = [];\n",
    "  test_error_history_nocovar = [];\n",
    "\n",
    "  training_error_history_withcovar = [];\n",
    "  validation_error_history_withcovar = [];\n",
    "  test_error_history_withcovar = [];\n",
    "\n",
    "\n",
    "  while (((test_error>test_error_thres) or (valid_error > valid_error_thres)) and iter < max_iters):\n",
    "    iter+=1;\n",
    "    \n",
    "    all_ind = set(np.arange(0,len(u_all_training)));\n",
    "    select_ind = np.random.randint(0,len(u_all_training),size=batchsize);\n",
    "    valid_ind = list(all_ind -set(select_ind))[0:batchsize];\n",
    "    select_ind_test = list(all_ind - set(valid_ind) - set(select_ind))[0:batchsize];\n",
    "\n",
    "    \n",
    "    u_batch =[];\n",
    "    u_control_batch = [];\n",
    "    y_batch = [];\n",
    "    u_valid = [];\n",
    "    u_control_valid = [];\n",
    "    y_valid = [];\n",
    "    u_test_train = [];\n",
    "    u_control_train = [];\n",
    "    y_test_train= [];\n",
    "    u_control_test_train = [];\n",
    "    \n",
    "    for j in range(0,len(select_ind)):\n",
    "      u_batch.append(u_all_training[select_ind[j]]);  \n",
    "#      y_batch = embed_feed.eval(feed_dict={u_feed:u_batch});\n",
    "      y_batch.append(y_all_training[select_ind[j]]);\n",
    "          \n",
    "    for k in range(0,len(valid_ind)):\n",
    "      u_valid.append(u_all_training[valid_ind[k]]);\n",
    "      y_valid.append(y_all_training[valid_ind[k]]);\n",
    "#      y_valid = embed_feed.eval(feed_dict={u_feed:u_valid});\n",
    "\n",
    "\n",
    "    for k in range(0,len(select_ind_test)):\n",
    "      u_test_train.append(u_all_training[select_ind_test[k]]);\n",
    "      y_test_train.append(y_all_training[select_ind_test[k]]);  \n",
    "#    y_test_train = embed_feed.eval(feed_dict={u_feed:u_test_train});\n",
    "    u_batch = np.asarray(u_batch);\n",
    "    y_batch = np.asarray(y_batch);\n",
    "    \n",
    "    optimizer.run(feed_dict={u_feed:u_batch,y_feed:y_batch,step_size:step_size_val});\n",
    "    valid_error = obj_func.eval(feed_dict={u_feed:u_valid,y_feed:y_valid});\n",
    "    test_error = obj_func.eval(feed_dict={u_feed:u_test_train,y_feed:y_test_train});\n",
    "\n",
    "    \n",
    "    if iter%samplerate==0:\n",
    "      training_error_history_nocovar.append(obj_func.eval(feed_dict={u_feed:u_batch,y_feed:y_batch}));\n",
    "      validation_error_history_nocovar.append(obj_func.eval(feed_dict={u_feed:u_valid,y_feed:y_valid}));\n",
    "      test_error_history_nocovar.append(obj_func.eval(feed_dict={u_feed:u_test_train,y_feed:y_test_train}));\n",
    "\n",
    "  \n",
    "      if (iter%10==0) or (iter==1):\n",
    "        #plt.close();                  \n",
    "        print (\"step %d , validation error %g\"%(iter, obj_func.eval(feed_dict={u_feed:u_valid,y_feed:y_valid})));\n",
    "        print (\"step %d , test error %g\"%(iter, obj_func.eval(feed_dict={u_feed:u_test_train,y_feed:y_test_train})));\n",
    "        #print(\"Reconstruction Loss: \" + repr(this_vae_loss.eval(feed_dict={this_u:this_corpus_vec})))\n",
    "#        this_corpus_embed = embed_feed.eval(feed_dict={u_feed:})\n",
    "        #print(\"Embedding Loss: \" + repr(this_embed_loss.eval(feed_dict={this_u:this_corpus_vec})) )\n",
    "    \n",
    "#    if ((iter>20000) and iter%10) :#\n",
    "#\n",
    "#      valid_gradient = np.gradient(np.asarray(validation_error_history_nocovar[iter/samplerate*7/10:]));\n",
    "#      mu_gradient = np.mean(valid_gradient);\n",
    "#\n",
    "#      if ((iter <1000) and (mu_gradient >= 5e-1)): # eventually update this to be 1/10th the mean of batch data, or mean of all data handed as input param to func\n",
    "#        good_start = 0; # if after 10,000 iterations validation error is still above 1e0, initialization was poor.\n",
    "#        print(\"Terminating model refinement loop with gradient:\") + repr(mu_gradient) + \", validation error after \" + repr(iter) + \" epochs:  \" + repr(valid_error);\n",
    "#        iter = max_iters; # terminate while loop and return histories\n",
    "\n",
    "  all_histories = [training_error_history_nocovar, validation_error_history_nocovar,test_error_history_nocovar];\n",
    "  \n",
    "  plt.close();\n",
    "  x = np.arange(0,len(validation_error_history_nocovar),1);\n",
    "  plt.plot(x,training_error_history_nocovar,label='train. err.');\n",
    "  plt.plot(x,validation_error_history_nocovar,label='valid. err.');\n",
    "  plt.plot(x,test_error_history_nocovar,label='test err.');\n",
    "  #plt.gca().set_yscale('log');\n",
    "  plt.savefig('all_error_history.pdf');\n",
    "  \n",
    "  plt.close();\n",
    "  return all_histories,good_start;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def vae_loss(y_model,y_true):\n",
    "        return tf.norm(y_true - y_model,axis=[0,1],ord='fro')#/tf.norm(y_true,axis=[0,1],ord='fro');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim_parameter = 2; \n",
    "#label_dim = 1; \n",
    "intermediate_dim = 100\n",
    "output_dim = Ydata.shape[1];\n",
    "batch_size_parameter=200;#4000 for howard's e. coli dataset\n",
    "debug_splash = 0;\n",
    "this_step_size_val = 0.025;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of hv_list: 3\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "n_depth: 3\n",
      "WARNING:tensorflow:From <ipython-input-4-51c51ed58bea>:65: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "step 5000 , validation error 568230\n",
      "step 5000 , test error 596747\n",
      "step 10000 , validation error 565953\n",
      "step 10000 , test error 613753\n",
      "step 15000 , validation error 583584\n",
      "step 15000 , test error 605608\n",
      "step 20000 , validation error 577935\n",
      "step 20000 , test error 547628\n",
      "step 25000 , validation error 567371\n",
      "step 25000 , test error 552021\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-fb1a745e5ff0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#this_embed_loss = embed_loss(this_u,this_embedding);\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtrain_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthis_y_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mUdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthis_u\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthis_io_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthis_optim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatchsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size_parameter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep_size_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthis_step_size_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-24d3a76e541f>\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(y_all_training, y_feed, u_all_training, u_feed, obj_func, optimizer, u_control_all_training, valid_error_thres, test_error_thres, max_iters, step_size_val, batchsize)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m#    y_test_train = embed_feed.eval(feed_dict={u_feed:u_test_train});\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mu_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mu_feed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mu_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_feed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstep_size_val\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_vars_list = [input_dim_parameter,intermediate_dim,output_dim];\n",
    "\n",
    "this_u = tf.placeholder(tf.float32, shape=[None,input_dim_parameter],name=\"InputInducers\");\n",
    "with tf.device('/cpu:0'):\n",
    "    this_W_list,this_b_list = initialize_Wblist(input_dim_parameter,hidden_vars_list);\n",
    "    this_y_out,all_layers = network_assemble(this_u,this_W_list,this_b_list,keep_prob=1.0,activation_flag=2,res_net=0)\n",
    "\n",
    "    this_y_true = tf.placeholder(tf.float32,shape=[None,output_dim],name=\"Groundtruth_Transcriptome\")    \n",
    "    this_output_layer = all_layers[-3]\n",
    "\n",
    "    result = sess.run(tf.global_variables_initializer())\n",
    "    this_io_loss = vae_loss(this_y_out,this_y_true)\n",
    "    this_optim = tf.train.AdagradOptimizer(learning_rate=this_step_size_val).minimize(this_io_loss)\n",
    "    step_size = tf.placeholder(tf.float32,shape=[],name=\"StepSizePlaceholder\");\n",
    "    result = sess.run(tf.global_variables_initializer())\n",
    "    #this_embed_loss = embed_loss(this_u,this_embedding);\n",
    "\n",
    "    train_net(Ydata,this_y_true,Udata,this_u,this_io_loss,this_optim,batchsize = batch_size_parameter,step_size_val = this_step_size_val,max_iters=5e4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YT*U*(U^T*U)^{-1} = G "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = np.matmul(Ydata.T,np.matmul(Udata,np.linalg.inv(np.matmul(Udata.T,Udata )) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
