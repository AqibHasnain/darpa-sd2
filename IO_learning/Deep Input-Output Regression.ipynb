{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np;\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math;\n",
    "import random;\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt;\n",
    "import pickle\n",
    "import sklearn.preprocessing as preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " all_error_history.pdf\r\n",
      " alt_data\r\n",
      "'Deep Input-Output Regression.ipynb'\r\n",
      " NAND_Titration_IO_data_empty_landing_pads_full.pickle\r\n",
      " NAND_Titration_IO_data_NAND_Circuit_full.pickle\r\n",
      " outputs.pdf\r\n",
      " RNAseq_merged.csv\r\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ydata shape:(480, 4110)\n",
      "Udata shape:(480, 2)\n"
     ]
    }
   ],
   "source": [
    "file_obj = open('NAND_Titration_IO_data_empty_landing_pads_full.pickle','rb')\n",
    "#file_obj = open('NAND_Titration_IO_data_MG1655_empty_landing_pads.pickle','rb')\n",
    "#file_obj = open('NAND_Titration_IO_data_MG1655_NAND_Circuit.pickle','rb')\n",
    "allVars = pickle.load(file_obj)\n",
    "Udata_raw = allVars[0]  #Output Data \n",
    "Ydata_raw = allVars[1]  #Input Data\n",
    "print(\"Ydata shape:\" + repr(Ydata_raw.shape));\n",
    "print(\"Udata shape:\" + repr(Udata_raw.shape));\n",
    "\n",
    "Ydata_raw = np.asarray(Ydata_raw,dtype=np.float32)\n",
    "\n",
    "Udata_raw = np.asarray(Udata_raw,dtype=np.float32)\n",
    "\n",
    "YmmS = preprocessing.MinMaxScaler(feature_range=(-1,1));\n",
    "UmmS = preprocessing.MinMaxScaler(feature_range=(0,1));\n",
    "Ydata = YmmS.fit_transform(Ydata_raw)\n",
    "Udata = UmmS.fit_transform(Udata_raw)\n",
    "#UmmS.inverse_transform(....)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=50)\n",
    "# pca.fit(Ydata)  \n",
    "# PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
    "#   svd_solver='auto', tol=0.0, whiten=False)\n",
    "# print(pca.explained_variance_ratio_)  \n",
    "# print(pca.singular_values_)  \n",
    "# Ydata = pca.transform(Ydata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expose_deep_basis(z_list,num_bas_obs,deep_dict_size,iter_num,u):\n",
    "    basis_hooks = z_list[-1]; #[-1] is y  = K *\\phi; -2 is \\phi(yk)\n",
    "    x_range = np.arange(-10.0,10.0,0.1);\n",
    "\n",
    "    for i in range(0,num_bas_obs):\n",
    "        plt.close();\n",
    "        scan_injection = np.zeros((len(x_range),num_bas_obs));\n",
    "        scan_injection[:,i]= np.transpose(x_range);\n",
    "        phi_j = basis_hooks.eval(feed_dict={u:scan_injection});\n",
    "        fig_hand = plt.gcf()\n",
    "        plt.plot(x_range,phi_j,'.-',label='\\phi_i(y)');\n",
    "        #plt.ylim([-2.0,2.0]);\n",
    "        fig = plt.gcf();\n",
    "        plt.savefig('deep_basis_images/phi_with_u' + repr(i) + '_iternum_' + repr(iter_num) + '.jpg');\n",
    "\n",
    "    return fig_hand;        \n",
    "\n",
    "def xavier_init(n_inputs, n_outputs, uniform=True):\n",
    "    \"\"\"Set the parameter initialization using the method described.\n",
    "    This method is designed to keep the scale of the gradients roughly the same\n",
    "    in all layers.\n",
    "    Xavier Glorot and Yoshua Bengio (2010):\n",
    "       Understanding the difficulty of training deep feedforward neural\n",
    "       networks. International conference on artificial intelligence and\n",
    "       statistics.\n",
    "    Args:\n",
    "    n_inputs: The number of input nodes into each output.\n",
    "    n_outputs: The number of output nodes for each input.\n",
    "    uniform: If true use a uniform distribution, otherwise use a normal.\n",
    "    Returns:\n",
    "    An initializer.\n",
    "    \"\"\"\n",
    "    if uniform:\n",
    "        # 6 was used in the paper.\n",
    "        init_range = math.sqrt(6.0 / (n_inputs + n_outputs))\n",
    "        return tf.random_uniform_initializer(-init_range, init_range)\n",
    "    else:\n",
    "        # 3 gives us approximately the same limits as above since this repicks\n",
    "        # values greater than 2 standard deviations from the mean.\n",
    "        stddev = math.sqrt(3.0 / (n_inputs + n_outputs))\n",
    "        return tf.truncated_normal_initializer(stddev=stddev)\n",
    "\n",
    "def weight_variable(shape):\n",
    "    std_dev = math.sqrt(3.0 /(shape[0] + shape[1]))\n",
    "    return tf.Variable(tf.truncated_normal(shape, mean=0.0,stddev=std_dev,dtype=tf.float32));\n",
    "  \n",
    "def bias_variable(shape):\n",
    "    std_dev = math.sqrt(3.0 / shape[0])\n",
    "    return tf.Variable(tf.truncated_normal(shape, mean=0.0,stddev=std_dev,dtype=tf.float32));\n",
    "\n",
    "\n",
    "def network_assemble(input_var,W_list,b_list,keep_prob=1.0,activation_flag=1,res_net=0):\n",
    "    n_depth = len(W_list);\n",
    "    print(\"n_depth: \" + repr(n_depth));\n",
    "    z_temp_list = [];\n",
    "    \n",
    "    for k in range(0,n_depth):\n",
    "        \n",
    "        if (k==0):\n",
    "            W1 = W_list[0];\n",
    "            b1 = b_list[0];\n",
    "            if activation_flag==1:# RELU\n",
    "                z1 = tf.nn.dropout(tf.nn.relu(tf.matmul(input_var,W1)+b1),keep_prob);\n",
    "            if activation_flag==2: #ELU \n",
    "                z1 = tf.nn.dropout(tf.nn.elu(tf.matmul(input_var,W1)+b1),keep_prob);\n",
    "            if activation_flag==3: # tanh\n",
    "                z1 = tf.nn.dropout(tf.nn.tanh(tf.matmul(input_var,W1)+b1),keep_prob);\n",
    "\n",
    "            z_temp_list.append(z1);\n",
    "            \n",
    "\n",
    "        if not (k==0) and k < (n_depth-1):\n",
    "            \n",
    "            prev_layer_output = tf.matmul(z_temp_list[k-1],W_list[k])+b_list[k]\n",
    "\n",
    "            if res_net and k==(n_depth-2):\n",
    "                prev_layer_output += tf.matmul(u,W1)+b1 #  this expression is not compatible for variable width nets (where each layer has a different width at inialization - okay with regularization and dropout afterwards though)\n",
    "\n",
    "            if activation_flag==1:\n",
    "                z_temp_list.append(tf.nn.dropout(tf.nn.relu(prev_layer_output),keep_prob));\n",
    "            if activation_flag==2:\n",
    "                z_temp_list.append(tf.nn.dropout(tf.nn.elu(prev_layer_output),keep_prob));\n",
    "            if activation_flag==3:\n",
    "                z_temp_list.append(tf.nn.dropout(tf.nn.tanh(prev_layer_output),keep_prob));\n",
    "\n",
    "                \n",
    "        if not (k==0) and k == (n_depth-1):\n",
    "            prev_layer_output = tf.matmul(z_temp_list[k-1],W_list[k])+b_list[k];\n",
    "            z_temp_list.append(prev_layer_output);\n",
    "\n",
    "        \n",
    "    if debug_splash:\n",
    "        print(\"[DEBUG] z_list\" + repr(z_list[-1]));\n",
    "        \n",
    "    #y_out = tf.concat([z_list[-1],u],axis=1); # last element of activation output list is the actual NN output\n",
    "    y_out = z_temp_list[-1];\n",
    "    \n",
    "    result = sess.run(tf.global_variables_initializer())\n",
    "    return y_out,z_temp_list;\n",
    "\n",
    "\n",
    "def initialize_Wblist(n_u,hv_list):\n",
    "    W_list = [];\n",
    "    b_list = [];\n",
    "    n_depth = len(hv_list);\n",
    "    print(\"Length of hv_list: \" + repr(n_depth))\n",
    "    #hv_list[n_depth-1] = n_y;\n",
    "    for k in range(0,n_depth):\n",
    "        \n",
    "        if k==0:\n",
    "            W1 = weight_variable([n_u,hv_list[k]]);\n",
    "            b1 = bias_variable([hv_list[k]]);\n",
    "            W_list.append(W1);\n",
    "            b_list.append(b1);\n",
    "        else:\n",
    "            W_list.append(weight_variable([hv_list[k-1],hv_list[k]]));\n",
    "            b_list.append(bias_variable([hv_list[k]]));\n",
    "    result = sess.run(tf.global_variables_initializer())\n",
    "    return W_list,b_list;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(y_all_training,y_feed,u_all_training,u_feed,obj_func,optimizer,u_control_all_training=None,valid_error_thres=1e-2,test_error_thres=1e-2,max_iters=100000,step_size_val=0.01,batchsize=10):\n",
    "\n",
    "  iter = 0;\n",
    "  samplerate = 5000;\n",
    "  good_start = 1;\n",
    "  valid_error = 100.0;\n",
    "  test_error = 100.0;\n",
    "  training_error_history_nocovar = [];\n",
    "  validation_error_history_nocovar = [];\n",
    "  test_error_history_nocovar = [];\n",
    "\n",
    "  training_error_history_withcovar = [];\n",
    "  validation_error_history_withcovar = [];\n",
    "  test_error_history_withcovar = [];\n",
    "\n",
    "\n",
    "  while (((test_error>test_error_thres) or (valid_error > valid_error_thres)) and iter < max_iters):\n",
    "    iter+=1;\n",
    "    \n",
    "    all_ind = set(np.arange(0,len(u_all_training)));\n",
    "    select_ind = np.random.randint(0,len(u_all_training),size=batchsize);\n",
    "    valid_ind = list(all_ind -set(select_ind))[0:batchsize];\n",
    "    select_ind_test = list(all_ind - set(valid_ind) - set(select_ind))[0:batchsize];\n",
    "\n",
    "    \n",
    "    u_batch =[];\n",
    "    u_control_batch = [];\n",
    "    y_batch = [];\n",
    "    u_valid = [];\n",
    "    u_control_valid = [];\n",
    "    y_valid = [];\n",
    "    u_test_train = [];\n",
    "    u_control_train = [];\n",
    "    y_test_train= [];\n",
    "    u_control_test_train = [];\n",
    "    \n",
    "    for j in range(0,len(select_ind)):\n",
    "      u_batch.append(u_all_training[select_ind[j]]);  \n",
    "#      y_batch = embed_feed.eval(feed_dict={u_feed:u_batch});\n",
    "      y_batch.append(y_all_training[select_ind[j]]);\n",
    "          \n",
    "    for k in range(0,len(valid_ind)):\n",
    "      u_valid.append(u_all_training[valid_ind[k]]);\n",
    "      y_valid.append(y_all_training[valid_ind[k]]);\n",
    "#      y_valid = embed_feed.eval(feed_dict={u_feed:u_valid});\n",
    "\n",
    "\n",
    "    for k in range(0,len(select_ind_test)):\n",
    "      u_test_train.append(u_all_training[select_ind_test[k]]);\n",
    "      y_test_train.append(y_all_training[select_ind_test[k]]);  \n",
    "#    y_test_train = embed_feed.eval(feed_dict={u_feed:u_test_train});\n",
    "    u_batch = np.asarray(u_batch);\n",
    "    y_batch = np.asarray(y_batch);\n",
    "    \n",
    "    optimizer.run(feed_dict={u_feed:u_batch,y_feed:y_batch,step_size:step_size_val});\n",
    "    valid_error = obj_func.eval(feed_dict={u_feed:u_valid,y_feed:y_valid});\n",
    "    test_error = obj_func.eval(feed_dict={u_feed:u_test_train,y_feed:y_test_train});\n",
    "\n",
    "    \n",
    "    if iter%samplerate==0:\n",
    "      training_error_history_nocovar.append(obj_func.eval(feed_dict={u_feed:u_batch,y_feed:y_batch}));\n",
    "      validation_error_history_nocovar.append(obj_func.eval(feed_dict={u_feed:u_valid,y_feed:y_valid}));\n",
    "      test_error_history_nocovar.append(obj_func.eval(feed_dict={u_feed:u_test_train,y_feed:y_test_train}));\n",
    "\n",
    "  \n",
    "      if (iter%10==0) or (iter==1):\n",
    "        #plt.close();                  \n",
    "        print (\"step %d , validation error %g\"%(iter, obj_func.eval(feed_dict={u_feed:u_valid,y_feed:y_valid})));\n",
    "        print (\"step %d , test error %g\"%(iter, obj_func.eval(feed_dict={u_feed:u_test_train,y_feed:y_test_train})));\n",
    "        #print(\"Reconstruction Loss: \" + repr(this_vae_loss.eval(feed_dict={this_u:this_corpus_vec})))\n",
    "#        this_corpus_embed = embed_feed.eval(feed_dict={u_feed:})\n",
    "        #print(\"Embedding Loss: \" + repr(this_embed_loss.eval(feed_dict={this_u:this_corpus_vec})) )\n",
    "    \n",
    "#    if ((iter>20000) and iter%10) :#\n",
    "#\n",
    "#      valid_gradient = np.gradient(np.asarray(validation_error_history_nocovar[iter/samplerate*7/10:]));\n",
    "#      mu_gradient = np.mean(valid_gradient);\n",
    "#\n",
    "#      if ((iter <1000) and (mu_gradient >= 5e-1)): # eventually update this to be 1/10th the mean of batch data, or mean of all data handed as input param to func\n",
    "#        good_start = 0; # if after 10,000 iterations validation error is still above 1e0, initialization was poor.\n",
    "#        print(\"Terminating model refinement loop with gradient:\") + repr(mu_gradient) + \", validation error after \" + repr(iter) + \" epochs:  \" + repr(valid_error);\n",
    "#        iter = max_iters; # terminate while loop and return histories\n",
    "\n",
    "  all_histories = [training_error_history_nocovar, validation_error_history_nocovar,test_error_history_nocovar];\n",
    "  \n",
    "  plt.close();\n",
    "  x = np.arange(0,len(validation_error_history_nocovar),1);\n",
    "  plt.plot(x,training_error_history_nocovar,label='train. err.');\n",
    "  plt.plot(x,validation_error_history_nocovar,label='valid. err.');\n",
    "  plt.plot(x,test_error_history_nocovar,label='test err.');\n",
    "  #plt.gca().set_yscale('log');\n",
    "  plt.savefig('all_error_history.pdf');\n",
    "  \n",
    "  plt.close();\n",
    "  return all_histories,good_start;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def vae_loss(y_model,y_true):\n",
    "        return tf.norm(y_true - y_model,axis=[0,1],ord='fro')#/tf.norm(y_true,axis=[0,1],ord='fro');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim_parameter = 2; \n",
    "#label_dim = 1; \n",
    "intermediate_dim = 100\n",
    "output_dim = Ydata.shape[1];\n",
    "batch_size_parameter=200;#4000 for howard's e. coli dataset\n",
    "debug_splash = 0;\n",
    "this_step_size_val = 0.25;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of hv_list: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0814 17:56:56.573663 140287656789824 deprecation.py:506] From <ipython-input-5-51c51ed58bea>:65: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0814 17:56:56.656692 140287656789824 deprecation.py:506] From /home/egbe290/anaconda3/envs/sd2/lib/python3.7/site-packages/tensorflow/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_depth: 3\n",
      "step 5000 , validation error 172.771\n",
      "step 5000 , test error 155.925\n",
      "step 10000 , validation error 173.141\n",
      "step 10000 , test error 173.506\n",
      "step 15000 , validation error 172.695\n",
      "step 15000 , test error 165.767\n",
      "step 20000 , validation error 166.497\n",
      "step 20000 , test error 171.103\n",
      "step 25000 , validation error 169.363\n",
      "step 25000 , test error 161.229\n",
      "step 30000 , validation error 165.476\n",
      "step 30000 , test error 163.46\n",
      "step 35000 , validation error 169.759\n",
      "step 35000 , test error 155.59\n",
      "step 40000 , validation error 173.597\n",
      "step 40000 , test error 160.673\n",
      "step 45000 , validation error 174.317\n",
      "step 45000 , test error 163.78\n",
      "step 50000 , validation error 167.813\n",
      "step 50000 , test error 156.564\n"
     ]
    }
   ],
   "source": [
    "hidden_vars_list = [input_dim_parameter,intermediate_dim,output_dim];\n",
    "\n",
    "this_u = tf.placeholder(tf.float32, shape=[None,input_dim_parameter],name=\"InputInducers\");\n",
    "for d in ['/device:GPU:2', '/device:GPU:3']:\n",
    "\n",
    "    with tf.device(d):\n",
    "    #with tf.device('/cpu:0'):\n",
    "        this_W_list,this_b_list = initialize_Wblist(input_dim_parameter,hidden_vars_list);\n",
    "        this_y_out,all_layers = network_assemble(this_u,this_W_list,this_b_list,keep_prob=1.0,\n",
    "                                                 activation_flag=2,res_net=0)\n",
    "\n",
    "        this_y_true = tf.placeholder(tf.float32,shape=[None,output_dim],name=\"Groundtruth_Transcriptome\")    \n",
    "        this_output_layer = all_layers[-3]\n",
    "\n",
    "        result = sess.run(tf.global_variables_initializer())\n",
    "        this_io_loss = vae_loss(this_y_out,this_y_true)\n",
    "        this_optim = tf.train.AdagradOptimizer(learning_rate=this_step_size_val).minimize(this_io_loss)\n",
    "        step_size = tf.placeholder(tf.float32,shape=[],name=\"StepSizePlaceholder\");\n",
    "        result = sess.run(tf.global_variables_initializer())\n",
    "        #this_embed_loss = embed_loss(this_u,this_embedding);\n",
    "\n",
    "        train_net(Ydata,this_y_true,Udata,this_u,this_io_loss,this_optim,\n",
    "                  batchsize = batch_size_parameter,step_size_val = this_step_size_val,max_iters=5e4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ydata = np.log10(Ydata+1e-15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YT*U*(U^T*U)^{-1} = G "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = np.matmul(Ydata.T,np.matmul(Udata,np.linalg.inv(np.matmul(Udata.T,Udata )) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_test = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "output = np.matmul(G,U_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yout = YmmS.inverse_transform(output.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster as cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yclust = cluster.AgglomerativeClustering(n_clusters=10).fit_predict(Yout.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newOrder = [[i,e] for i,e in enumerate(Yclust)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nS = np.array(newOrder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nS.sort(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dend = shc.dendrogram(shc.linkage(data_scaled, method='ward'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dend = hierarchy.dendrogram(hierarchy.linkage(Yout.T,method='ward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yout.T[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yplot = Yout.T\n",
    "Yplot[Yplot < 0] = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dfM = pd.read_csv('RNAseq_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geneNames = dfM.columns[7:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfG = pd.DataFrame(Yplot,index=geneNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfG.loc[['Sensor_LacI','Sensor_AraC','Circuit_PhlF','Circuit_IcaR','Actuator_YFP']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,20))\n",
    "sns.heatmap(np.log10(Yplot),cmap='viridis')\n",
    "\n",
    "plt.savefig('outputs.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yout.T[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "this_xlabels = ['arabinose','iptg']\n",
    "#this_ylabels = ['gene'+repr(ind) for ind in range(0,4110)]\n",
    "sns.set(font_scale=1.4)\n",
    "#hm=sns.heatmap(G,xticklabels=this_xlabels,#yticklabels=this_ylabels,cmap='RdYlGn',annot=False)\n",
    "hm=sns.heatmap((np.log10(G)+1e-15),xticklabels=this_xlabels,cmap='RdYlGn',annot=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
